% Strategies for
  Handling Missing Data
% Stephen Haptonstahl
  Berico Technologies
% 2012/08/30

# The Problem

## Data, in theory

<!-- 
Compile to HTML5 with: 
  
  pandoc -i -s -S -toc --latexmathml -t slidy -o HandlingMissingData.html HandlingMissingData.md 
  
Compile to PDF of Beamer slides with:
  
  pandoc -i -s -S -toc -t beamer -o HandlingMissingData.pdf HandlingMissingData.md

-->

Suppose we want to mine sales records to see what kind of customer buys more from us.

 sales  female   education     income
------ -------- -----------   -------
  1209    0       high school 35000
  2587    1       4yr degree   45000
 13265    1       grad degree  65000
  2298    0       some college 49000

## Data, in practice

Suppose we want to mine sales records to see what kind of customer buys more from us.

 sales  female   education     income
------ -------- -----------   -------
  1209    0       high school  35000
  2587    NA      4yr degree   45000
 13265    1       NA           65000
    NA    0       some college 49000

## Who cares?

* Best case if you ignore missing data: throwing away data
* Same as loss of efficiency
* Less certainty (less statistical significance)
* **Some patterns will be completely missed**
* Data scientists are replaced by equally incompetent algorithms, ie. 
* Mass hysteria

## Today's talk

<!-- * [The Problem](#The-Problem) -->

* The Problem
* Typical Method Versus a Better Method
* Naive Imputation
* Home-Rolled Imputation
* Existing Tools
    * Small Data Sets
    * Large Data Sets
    * Real-time
* When Imputation Fails
* Fin

# Typical Method Versus a Better Method

## (The Evil that is) Row Deletion

* Idea: Only include full rows in the analysis.
* This is the most common method.
    * Default for most software, inluding <tt>lm</tt> and other <tt>R</tt> functions.
* Throws away data, perhaps a lot.
      * Ex: 50 fields for each customer
      * How many have all 50 observed? (Ans: none except test data)
* **Some patterns will be completely missed**
* We know where this leads

## What is imputation?

Imputation

:    Filling in missing data with guesses, good or bad, for what would have been observed.

* Example 1: "These scientists all say the earth is warming, but today it's cold.  They must be in a conspiracy!"
* Example 2: "These scientists all say the earth is warming, but today it's cold.  Perhaps they only measured on warm days."
* Both are wrong, but one is a better (more likely to be true) guess than the other.
* The better guess will be less likely to lead you to bad decisions, algorithms, hysteria, etc.

## When is imputation bad

> If only we knew what was there.  Then we would be able to make a good guess at what was there.

* Missingness process **not ignorable**, which means
* Missingness depends on unobserved data
* Types of data-generating processes
    1. Missing Completely At Random (MCAR): *no data* will help you predict which values are missing
        * Random noise
    2. Missing at Random (MAR): *observed values* can help you predict which values are missing
        * Faulty switches
    3. Not ignorable (NMAR): *unobserved values* can help you predict which values are missing
        * Losses reported as zero income (censored)
* Imputing when missingness is not ignorable leads to bias, bad decisions, etc.
* cf. Donald Rubin (1976)

# Naive Imputation

## Mean, Median, Mode

> We just filled in the missing values with zero.

How we guess (hopefully) depends on what we expect to be there.

* Continuous variable
    * floating point
    * Use <tt>mean</tt>
* Ordinal (disagree, neither agree nor disagree, agree): <tt>median</tt>
* Count (or other integer): <tt>median</tt>
* Categorical (states; colors): mode
    
    <tt>StatMode <- function(x) names(table(x))[which.max(table(x))]</tt>

## Mean, Median, Mode (continued)

Suppose we want to mine sales records to see what kind of customer buys more from us.

 sales  female   education     income
------ -------- -----------   -------
  1209    0       high school  35000
  2587    NA      4yr degree   45000
 13265    1       NA           65000
    NA    0       some college 49000

## Mean, Median, Mode (continued)

Suppose we want to mine sales records to see what kind of customer buys more from us.

 sales  female   education               income
------ -------- -----------             -------
  1209       0       high school          35000
  2587       0       4yr degree           45000
 13265       1       some college         65000
  5687       0       some college         49000
  
  mean     mode      median
------ -------- -----------             -------

## Mean, Median, Mode (continued)

* Pros
    * Fast
    * Principled
    * Better than throwing away data
    * Tends to work
* Cons
    * We can do better.
    * "Making up data" makes us more certain than we should be.

# Home-Rolled Imputation

## Imputation with Regression

1. Identify a cell <tt>X[i,j]</tt> to impute.
2. Find all rows `prediction.rows` that:
    a. have an observed value for column <tt>j</tt>
    b. have observed values for all other columns <tt>obs.cols</tt> that row <tt>i</tt> has
3. Using those rows: <tt>reg <- lm(X[prediction.rows,j] ~ X[prediction.rows,obs.cols])</tt>
4. Fill with <tt>predict(reg, names(X)[obs.cols]=X[i,obs.cols])</tt>

## Imputation with Regression (continued)

     y   &nbsp;     x1     x2
 -----   ------  -----  -----
  7.44            1.21   1.95
  3.25           -1.35   0.11
  3.90              NA   0.63
  5.19            0.16   2.27
  5.57            0.17   1.53
  ...              ...    ...
 -----   ------  -----  -----
 
## Imputation with Regression (continued)

     y   &nbsp;     x1     x2
 -----   ------  -----  -----
  7.44            1.21   1.95
  3.25           -1.35   0.11
  3.90           -0.89   0.63
  5.19            0.16   2.27
  5.57            0.17   1.53
  ...              ...    ...
 -----   ------  -----  -----
 
* Missing value: -0.89
* Predicted value: -0.89

## Type of Regression Varies by Variable Type

Continuous

:    Linear regression with <tt>lm()</tt>

Ordinal

:    Ordered logit/probit regression with <tt>MASS::polr</tt>

Count

:    Poisson regression with <tt>glm(..., family=poisson(link = "log"))</tt>

Categorical

:    Multinomial logit with <tt>nnet::multinom</tt>

## Safety First!

* Look out for having too little data to estimate. (Default to mean/median/mode?)
* Data is dirty in other ways (illegal values).
* Iterative algorithms sometimes don't converge.
* Still have problem that "Making up data" makes us more certain than we should be.

# Existing Tools

## Small Data Sets

<tt>Amelia</tt> (King) and <tt>mi</tt> (Gelman) work differently from home-rolled solutions.

Multiple Imputation

1. Generate $m$ copies of the data set, each with **different** imputed values.
2. Perform the desired analysis on each imputed data set to get the quantity of interest $q$.
3. Aggregate the results:
    1. If $m=5$ or so
        * Point-estimates: <tt>mean(q)</tt>
        * SEs: fairly simple formula
    2. If $m$ large
        * Treat as simulation result
        * <tt>mean</tt> and <tt>sd</tt>
4. Check distribution of observed versus imputed values (<tt>plot</tt>, <tt>ks.test</tt>)

## Amelia Example

```{r Amelia1, message=FALSE, size='scriptsize'}
library(Amelia)
data(africa)
```

```{r Amelia2, size='footnotesize'}
a.out <- amelia(x = africa, cs = "country", ts = "year", logs = "gdp_pc")
```

## Amelia Example (continued)

```{r Amelia3, size='scriptsize'}
summary(a.out)
```

## Amelia Example (continued)

```{r Amelia4, fig.height=3, fig.width=4, echo=FALSE}
original.par <- par(no.readonly = TRUE)
par(omi=c(0,0,0,0), mar=c(2,1,1,.25), ps=8, tcl=-.3, mgp=c(.4, .4, 0), yaxt="n")
plot(a.out, xlab="", ylab="Density")
par(original.par)
```

## That's hard. What have we gained or lost?

* Correct reporting of uncertainty (SEs)
* Actually, it's easier than rolling your own
    * Smart about pathologies
    * Smart about variable types
    * Code is shorter if you are implementing in <tt>R</tt>
    * Very fast on reasonably sized data sets
* If you are **not** implementing in <tt>R</tt>, you have a lot more work
    * Reading the paper(s), implementing your own version
    * Quite do-able, not for faint at heart
* Not scalable

## Large Data Sets: Bayes

Bayesian data augmentation

* When using Bayes to estimate a model, put a prior on the *data*.
* The algorithm will give draws from the posterior distribution of the missing values.
* It uses the model defined (whatever it is) to impute.

## Large Data Sets: Bayes (continued)

Problems 

* Slow
* Convergence can be tricky
* Expertise required for custom samplers, but libraries for implementing custom solutions: <tt>rcppbugs</tt> in R, others for C++, Python, etc.

## Large Data Sets: Bayes (continued)

Example: Using Bayes for IRT on network links to generate DILS (data-informed link strength)

 linkid   net1   net2   net3   ...   netk 
-------- ------ ------ ------ ----- ------
  1       1      0      0      ...   1
  2       0      0      1      ...   0
  3       1      NA     1      ...   1
  4       1      0      NA     ...   0
  5       NA     0      0      ...   1

becomes...

## Large Data Sets: Bayes (continued)

Example: Using Bayes for IRT on network links to generate DILS (data-informed link strength)

 linkid  dils
-------- -----
  1       .12
  2       .01
  3       .34
  4       .08
  5       .03

## Large Data Sets: Random Forest Classifier

Easy solution: use <tt>missForest</tt>

```{r missForest1, message=FALSE, size='scriptsize'}
library(Amelia)  # to provide the data
data(africa)
library(missForest)
```

```{r missForest2, size='footnotesize'}
africa.rf <- missForest(africa)
```

* Keep **all** of the trees in the forest.
* Aggregate as with other simulations

## Large Data Sets: Random Forest Classifier (continued)

* Scales well.
* Forest gives us reasonable measures of uncertainty.
* Requires a **lot** of data (think non-parametric).
* Without care might give silly values.
* Still should check imputed distributions against observed distributions.

## Real-Time: <tt>FastImputation</tt>

* Project: Improve fraud detection classifier
* Same model as Amelia
* Learns (slowly) using lots of data
* Imputes a single row at a time (very, well, fast)
* On CRAN
* Caveat emptor (as with all <tt>R</tt> packages)

# When Imputation Fails

## To Imputate or Not?

* Assumptions of Imputation
    * Data is MCAR or MAR+PD
    * "Small" fraction of data missing
        * < 10%: usually no problem
        * 10%-20%: Be careful
        * > 20%: There be dragons
* Always^*^ better than row deletion

## What to Do When Imputation Fails

* Explicitly model the reason observations are missing
    * Tobit (incomes censored, displayed as zero)
    * Truncated regression (heights in military)
    * Abstentions in voting (more likely if you disagree with party)
    
    or
    
* Impute anyway and warn the consumer (worth considering)

# Fin

## Links and Contact Info

* This presentation and code: https://github.com/shaptonstahl/MissingData_2012-08
* <tt>Amelia</tt>: http://gking.harvard.edu/amelia (the 2001 paper is a good place to start)
* <tt>mi</tt>: http://cran.r-project.org/web/packages/mi
* Great ref on missing data, imputation: http://gking.harvard.edu/files/evil.pdf
* <tt>FastImputation</tt>: http://cran.r-project.org/web/packages/FastImputation
* Data-Informed Link Strength (DILS): https://github.com/shaptonstahl/dils
* Great place to work: http://www.bericotechnologies.com
* Stephen Haptonstahl -- srh@haptonstahl.org -- @polimath
